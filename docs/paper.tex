\documentclass[11pt,twoside,a4paper]{article}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother

\begin{document}
\title{Blinkered Gradient Boosted Trees}
\author{John Hawkins}
\date{December 2017}
\maketitle

\begin{abstract}

Gradient boosting is an iterative ensembling technique for building a model from
simple base learners. By making several non-standard assumptions about the way noise is often
distributed in real world data, we are able to derive a simple variation of
the gradient boosting algorithm with demonstrable utility. 
We conduct experiments with several public data sets
and show that our approach can result in improved accuracy when compared with 
the standard gradient boosting implementation. While the approach is not a universal 
improvement, due to its unique treatment of specific kinds of noise it 
represents a novel tool for machine learning practitioners.

\end{abstract}

\section{Introduction}

Gradient Boosting has proven itself as one of the most significant advances in
the field of machine learning in recent history. So much so that for a period
of time you could garantee that the winner of every Kaggle competition was using
either a deep learning neural network or an ensemble that contained gradient
boosted trees \cite{}. In spite of its success, there are a range of specific situations 
in which gradient boosting is known to have problems. The most notable being the 
presence of noise, which will cause the model to expend training effort attempting
to fit the noise \cite{}.

Many variations of the gradient boosting algorithm have appeeared, each of which 
includes additional ensemble modelling techniques designed to lower the bias and
or variance of the algorithm, such as regularization, bootstrap aggregating or
variations in the CART algorithm used in the base learners.
For example XGBoost\cite{}, LiteBoost and .

However, in all cases the theoretical underpinnings of the gradient boosting algorithm
remain unchanged. The algorithm consists of learning an ensemble of base learners
as an additive combination. In each iteration the new model is trained to predict the
residuals of the preceding ensemble. Each weak learner is added to the ensemble with
a small weighting, usually called a learning rate or shrinkage, which prevents over-fitting. 

In this paper we develop a variation of the gradient boosting algorithm by making
several non-standard assumptions about the noise present in the data. 
Firstly we assume that the noise is not randomly distributed with respect to the 
features or predictors. This assumption is contrary to the standard expression for the learning problem, 
an example of which is shown in Equation \ref{eq:noise} taken from Friedman 1999 \cite{Friedman1999}.

\begin{equation}
y_i = F^*(x_i) + \epsilon_i
\label{eq:noise}
\end{equation}

This expression defines the target variable of instance $y_i$ as a function of the independent variables
in the features vector $x$ plus some Gaussian noise $\epsilon_i$. 
The noise term may represent noise inherent in the problem, or noise due to
the underlying physical process being understermined by the available independent variables.
It is generally assumed that the noise term is independent of the feature vector X. 
We propose breaking this assumption because in real world processes there are multiple reasons
why the distribution of the noise might be correlated with predictors. 
Noise can come from sampling problems, however in many real world data sets with sampling issues
you will find that the shortage of samples tends to be localised to a particular region of the 
feature space.
Noise may also be due to limitations of the devices or methodology used to collect data, again
a kind of sample problem, that will likely be restricted to certain ranges of feature values.
Most noise is due to some physical process, which itself is regulated by variables,
if any of those variables are present as features then we should expect the statistical 
structure of the noise to vary over the features space.

This means that we should expect the learning problem in many real world problems to be

\begin{equation}
y_i = F^*(x_i) + \epsilon(N(x_i))
\label{eq:noise}
\end{equation}

In which $\epsilon(N(x_i))$ refers to a sample drawn from a noise distribution $N$ that is 
parameterised by the specific variables of the sample $x_i$.

We can now ask how such an assumption changes our expectations about the errors made by a
learning algorithm, and how we should adapt the algorithm to accomodate noise of this kind.

Learning algorithms generally involve a tension between fitting a function to the structure of
the training data, and controlling its complexity so that it will generalise beyond the training
data. The problem with fitting the training data perfectly is twofold. firstly most training sets
will be an incomplete representation of the task to be learned. This is essense of the bias/variance
dilemna, complex models with low bias can fit a give data set in a multitude of ways, resulting in
high variance. The second problem with fitting the training data perfectly stems from the presence of noise.

In both cases the solution is to control the complexity of the model through some form of regularization.

If we presume that the learning algorithm is unbiased in its ability to learn the underlying
(non-noise) part of the relationship between the features and response, then we would expect
under this new defintion that the error made by the learning algorithm will not be evenly 
distributed across the feature space. Instead it will be weighted toward regions of the feature
space that result in more noise due to the sensitivity of $N$ to the values in $x_i$.

This leads us to assume that there will be some correlation between the magnitude of the error made
by the model and the likelihood that this error is due to the presence of noise. 
This assumption requires only that our feature vector and model be rich enough that we expect the 
model to be able to learn the underlying relationship between dependent and independent variables 
well enough that noisier samples stand out. In the absence of noise we expect error to be low, 
when noise is present we expect it to be high. Because our change to the learning problem leads us
to expect us non-uniform noise, we therefore expect the presence of a noise gradient that is correlated with the error.

Combining these two assumptions and the general idea of a gradient boosting leads us to
an alternative learning algorithm we call Blinkered Gradient Boosting. 
We draw on the analogy of a racehorse wearing 
leather blinkers which prevent if from being distracted from activity on the edge
of the racecourse. The metaphorical algorithmic blinkers prevent the model from being
distracted by the large residual errors that are likely due to noise and hence unsolvable.
It is worth noting the idea of minimising the influence of outliers in the error function is 
not new, it is the reason for the use of the Huber loss function \cite{}. However, gradient boosted
methods generally assume that any error from previous iterations of the learning algorithm is
due to a poor fit and need to be corrected. This is reason gradient boosting can be sensitive to
noise.
 

\section{Method}

The core idea behind blinkered gradient boosting is that at each iteration of the training
algorithm we exclude some portion of the samples due to the fact that their error is beyond
the limits set by the blinkers. These blinkers are set at an outer bound with a new meta-paramter.
They are gradually reduced in size by factor set by an additional meta-parametrs, or set to 
reduce by an amount set by the number of training epochs.

In principle we could use values for these blinkers that are assymetric around zero, however because
all standard loss functions are symetric around zero we use a single blinker value that applies to positive
and negative eror.

As well as excluding certain examples from the training data, we also train a binary classifier
to predict whether a given example belongs to the set of data points whose error will be within those
bounds. Ideally, as we apply the sequence of models to new data we do not want to attempt to correct
predictions whose error is likley to be due to noise. We include these classifiers as an optional
aspect of the algorithm. The user can choose whether to include them in the learning and scoring
of data.

The most general version of the algorithm is shown below, as an adaptation of the algorithm described by
Friedman 1999 \cite{firedman_1999}.


\begin{algorithm}
\caption{Blinkered Gradient Boost}\label{bgm}
\begin{algorithmic}[1]
\State $\displaystyle F_0(x) = arg min_{\rho} \sum_{i=1}^N \mathcal{L}(y_i, \rho) $
\For {$m$ = 1 to $M$}
\State $B_m = \mathcal{B} - (m-1)\delta $
\State $\displaystyle \Omega = \left\{ i \in [1,N] \ | \ \mathcal{L}(y_i, F_{m-1}(x_i)) < B_m \right\} $
\State $\displaystyle \tilde{y}_i = - \left[ \frac{\partial \mathcal{L}(y_i, F(x_i)) }{\partial  F(x_i)} \right]_{F(x) =F_{m-1}(x)}, i \in \Omega  $
\State $\displaystyle a_m = arg min_a \sum_{i \in \Omega} \mathcal{L}(\tilde{y}_i, h(x_i, a)) $
\EndFor
\end{algorithmic}
\end{algorithm}

 
Following from Friedman we define the algorithm in terms of an arbitrary loss function, illustrating
that the technique is flexible and can be applied to any problem in which the notion of feature dependent
noise is deemed appropriate. Our implementation is specific to our purposes, which is the forecasting
of timeseries data with a large number of external regressors.

We implement our method as an S4 class in R, that wraps around a collection of H2O regression trees.
H2O was chosen to provide the base learners because we have been dealing with large distributed data 
sets and required a method to implement and test algorithms on these data sets.

\section{Data Sets}

We extract 5 data sets from the UCI machine learning repository. We chose sets for which there were greater than ten thousand
examples and more than 20 features, because we are looking for data sets that might fullfill the conditions for which the
algorithm is designed. 

Those data sets with their respective statistics are shown in Table XX.

  

\section{Evaluation}

We take three large regression data sets that are publically available 
and apply our technique alongside the gradient boosting implementation
provided with H2O. We use default parameters and several common alternatives
for the sake of a rounded comparison. 

We evaluate the performance using Mean Absolute Error on a hold out data set.
The models are trained using a early stopping on an validation data set. Both
the validation and hold out test data were created as out-of-time data to reflect
the kinds of real world processes we are interested in. 

The code for the evaluation is provided in our GitHub repository. 


Note, that due to liscensing restrictions we are unable to dsitribute the data sets
themselves. In order to run
it you need to download the public data sets from their sources (shown in Table \ref{}).

Lending CLub
https://www.lendingclub.com/info/download-data.action

Walmart Sales Data
https://www.kaggle.com/c/walmart-recruiting-store-sales-forecasting/data


\section{Results}



\section{Conclusion}

By questioning a single standard assumption about the nature of the noise present in a data set
we have been able to derive an variation of the gradient boosting algorithm. We have then showed
using a selection of data sets that this model has utility for real world data.


  \begin{thebibliography}{1}

  \bibitem{Friedman1999} Jerome Friedman {\em Greedy Function Approximation: A Gradient Boosting Machine.}  1999.

  \bibitem{notes} John W. Dower {\em Readings compiled for History
  21.479.}  1991.

  \bibitem{impj}  The Japan Reader {\em Imperial Japan 1800-1945} 1973:
  Random House, N.Y.

  \bibitem{norman} E. H. Norman {\em Japan's emergence as a modern
  state} 1940: International Secretariat, Institute of Pacific
  Relations.

  \bibitem{fo} Bob Tadashi Wakabayashi {\em Anti-Foreignism and Western
  Learning in Early-Modern Japan} 1986: Harvard University Press.

  \end{thebibliography}

\end{document}

